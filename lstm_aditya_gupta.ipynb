{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ”¥Natural Language ProcessingðŸ”¥ </div>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ“‚Importing DependenciesðŸ“‚ </div>\n\n\n\n   We shall start by importing all the neccessary libraries. I will explain the exact use of each library later in this notebook.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport re\n\nprint(\"Tensorflow Version\",tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-06-05T12:23:46.756447Z","iopub.execute_input":"2024-06-05T12:23:46.757379Z","iopub.status.idle":"2024-06-05T12:23:59.545549Z","shell.execute_reply.started":"2024-06-05T12:23:46.757337Z","shell.execute_reply":"2024-06-05T12:23:59.544638Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-05 12:23:48.381504: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-05 12:23:48.381606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-05 12:23:48.497998: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nTensorflow Version 2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ› Dataset PreprocessingðŸ›  </div>\n  \n  \nIn this notebook, I am using **Sentiment-140** from [Kaggle](https://www.kaggle.com/kazanova/sentiment140). It contains a labels data of 1.6 Million Tweets and I find it a good amount of data to train our model.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n                 encoding = 'latin',header=None)\ndf.sample(6).style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'})","metadata":{"execution":{"iopub.status.busy":"2024-06-05T12:24:11.642388Z","iopub.execute_input":"2024-06-05T12:24:11.643383Z","iopub.status.idle":"2024-06-05T12:24:17.580783Z","shell.execute_reply.started":"2024-06-05T12:24:11.643347Z","shell.execute_reply":"2024-06-05T12:24:17.579766Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x79aba2b154b0>","text/html":"<style type=\"text/css\">\n#T_0d452_row0_col0, #T_0d452_row0_col1, #T_0d452_row0_col2, #T_0d452_row0_col3, #T_0d452_row0_col4, #T_0d452_row0_col5, #T_0d452_row1_col0, #T_0d452_row1_col1, #T_0d452_row1_col2, #T_0d452_row1_col3, #T_0d452_row1_col4, #T_0d452_row1_col5, #T_0d452_row2_col0, #T_0d452_row2_col1, #T_0d452_row2_col2, #T_0d452_row2_col3, #T_0d452_row2_col4, #T_0d452_row2_col5, #T_0d452_row3_col0, #T_0d452_row3_col1, #T_0d452_row3_col2, #T_0d452_row3_col3, #T_0d452_row3_col4, #T_0d452_row3_col5, #T_0d452_row4_col0, #T_0d452_row4_col1, #T_0d452_row4_col2, #T_0d452_row4_col3, #T_0d452_row4_col4, #T_0d452_row4_col5, #T_0d452_row5_col0, #T_0d452_row5_col1, #T_0d452_row5_col2, #T_0d452_row5_col3, #T_0d452_row5_col4, #T_0d452_row5_col5 {\n  background-color: #f9f9f9;\n  color: #4CAF50;\n  font-weight: bold;\n}\n</style>\n<table id=\"T_0d452\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_0d452_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n      <th id=\"T_0d452_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n      <th id=\"T_0d452_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n      <th id=\"T_0d452_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n      <th id=\"T_0d452_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n      <th id=\"T_0d452_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_0d452_level0_row0\" class=\"row_heading level0 row0\" >302868</th>\n      <td id=\"T_0d452_row0_col0\" class=\"data row0 col0\" >0</td>\n      <td id=\"T_0d452_row0_col1\" class=\"data row0 col1\" >1999200157</td>\n      <td id=\"T_0d452_row0_col2\" class=\"data row0 col2\" >Mon Jun 01 19:48:46 PDT 2009</td>\n      <td id=\"T_0d452_row0_col3\" class=\"data row0 col3\" >NO_QUERY</td>\n      <td id=\"T_0d452_row0_col4\" class=\"data row0 col4\" >MriaJo</td>\n      <td id=\"T_0d452_row0_col5\" class=\"data row0 col5\" >praying for the people of the Air France flight.. ! </td>\n    </tr>\n    <tr>\n      <th id=\"T_0d452_level0_row1\" class=\"row_heading level0 row1\" >1369583</th>\n      <td id=\"T_0d452_row1_col0\" class=\"data row1 col0\" >4</td>\n      <td id=\"T_0d452_row1_col1\" class=\"data row1 col1\" >2050830027</td>\n      <td id=\"T_0d452_row1_col2\" class=\"data row1 col2\" >Fri Jun 05 19:22:07 PDT 2009</td>\n      <td id=\"T_0d452_row1_col3\" class=\"data row1 col3\" >NO_QUERY</td>\n      <td id=\"T_0d452_row1_col4\" class=\"data row1 col4\" >mcFATTIE</td>\n      <td id=\"T_0d452_row1_col5\" class=\"data row1 col5\" >Got my internet working. </td>\n    </tr>\n    <tr>\n      <th id=\"T_0d452_level0_row2\" class=\"row_heading level0 row2\" >1304640</th>\n      <td id=\"T_0d452_row2_col0\" class=\"data row2 col0\" >4</td>\n      <td id=\"T_0d452_row2_col1\" class=\"data row2 col1\" >2010586553</td>\n      <td id=\"T_0d452_row2_col2\" class=\"data row2 col2\" >Tue Jun 02 17:37:55 PDT 2009</td>\n      <td id=\"T_0d452_row2_col3\" class=\"data row2 col3\" >NO_QUERY</td>\n      <td id=\"T_0d452_row2_col4\" class=\"data row2 col4\" >MistyMontano</td>\n      <td id=\"T_0d452_row2_col5\" class=\"data row2 col5\" >If the rain picks up  &amp; you see any flooding, please let me know! thank you </td>\n    </tr>\n    <tr>\n      <th id=\"T_0d452_level0_row3\" class=\"row_heading level0 row3\" >1022156</th>\n      <td id=\"T_0d452_row3_col0\" class=\"data row3 col0\" >4</td>\n      <td id=\"T_0d452_row3_col1\" class=\"data row3 col1\" >1882736793</td>\n      <td id=\"T_0d452_row3_col2\" class=\"data row3 col2\" >Fri May 22 07:23:56 PDT 2009</td>\n      <td id=\"T_0d452_row3_col3\" class=\"data row3 col3\" >NO_QUERY</td>\n      <td id=\"T_0d452_row3_col4\" class=\"data row3 col4\" >mandylulu</td>\n      <td id=\"T_0d452_row3_col5\" class=\"data row3 col5\" >@JJRogue thank you..my fat cat happy side appreciates the hard work u put in so I don't have to..keep it up champ! </td>\n    </tr>\n    <tr>\n      <th id=\"T_0d452_level0_row4\" class=\"row_heading level0 row4\" >1552302</th>\n      <td id=\"T_0d452_row4_col0\" class=\"data row4 col0\" >4</td>\n      <td id=\"T_0d452_row4_col1\" class=\"data row4 col1\" >2184367539</td>\n      <td id=\"T_0d452_row4_col2\" class=\"data row4 col2\" >Mon Jun 15 15:48:25 PDT 2009</td>\n      <td id=\"T_0d452_row4_col3\" class=\"data row4 col3\" >NO_QUERY</td>\n      <td id=\"T_0d452_row4_col4\" class=\"data row4 col4\" >cashexpert</td>\n      <td id=\"T_0d452_row4_col5\" class=\"data row4 col5\" >@hectichelpers congrats on the good interview, you were very calm and collected when the boys were, well being boys! </td>\n    </tr>\n    <tr>\n      <th id=\"T_0d452_level0_row5\" class=\"row_heading level0 row5\" >733044</th>\n      <td id=\"T_0d452_row5_col0\" class=\"data row5 col0\" >0</td>\n      <td id=\"T_0d452_row5_col1\" class=\"data row5 col1\" >2264216718</td>\n      <td id=\"T_0d452_row5_col2\" class=\"data row5 col2\" >Sun Jun 21 03:09:13 PDT 2009</td>\n      <td id=\"T_0d452_row5_col3\" class=\"data row5 col3\" >NO_QUERY</td>\n      <td id=\"T_0d452_row5_col4\" class=\"data row5 col4\" >Dress4Less</td>\n      <td id=\"T_0d452_row5_col5\" class=\"data row5 col5\" >@duskyblueskies most woman are A cups so the stores run out of that size first   Bad buyers!!!</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"You can see the columns are without any proper names. Lets rename them for our reference","metadata":{}},{"cell_type":"code","source":"df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf.head().style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'})","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:19.147799Z","iopub.execute_input":"2024-06-01T03:34:19.148238Z","iopub.status.idle":"2024-06-01T03:34:19.159364Z","shell.execute_reply.started":"2024-06-01T03:34:19.148212Z","shell.execute_reply":"2024-06-01T03:34:19.158434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to train only on text to classify its sentiment. So we can ditch the rest of the useless columns.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:19.161999Z","iopub.execute_input":"2024-06-01T03:34:19.162866Z","iopub.status.idle":"2024-06-01T03:34:19.204051Z","shell.execute_reply.started":"2024-06-01T03:34:19.162842Z","shell.execute_reply":"2024-06-01T03:34:19.202831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\ndef label_decoder(label):\n  return lab_to_sentiment[label]\ndf.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\ndf.head().style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'})","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:19.205296Z","iopub.execute_input":"2024-06-01T03:34:19.205831Z","iopub.status.idle":"2024-06-01T03:34:19.808359Z","shell.execute_reply.started":"2024-06-01T03:34:19.205799Z","shell.execute_reply":"2024-06-01T03:34:19.807483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are decoding the labels. We map **0 -> Negative and 1 -> Positive** as directed by the datset desciption. Now that we decoded we shall now analyse the dataset by its distribution. Because it's important that we have almost small amount of examples for given classes.","metadata":{}},{"cell_type":"code","source":"val_count = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:19.80959Z","iopub.execute_input":"2024-06-01T03:34:19.809933Z","iopub.status.idle":"2024-06-01T03:34:20.230032Z","shell.execute_reply.started":"2024-06-01T03:34:19.809903Z","shell.execute_reply":"2024-06-01T03:34:20.229121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's a very good dataset without any skewness. Thank Goodness.\n\nNow let us explore the data we having here... ","metadata":{}},{"cell_type":"code","source":"import random\nrandom_idx_list = [random.randint(1,len(df.text)) for i in range(10)] # creates random indexes to choose from dataframe\ndf.loc[random_idx_list,:].head(10).style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'}) # Returns the rows with the index and display it","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:20.231298Z","iopub.execute_input":"2024-06-01T03:34:20.231676Z","iopub.status.idle":"2024-06-01T03:34:20.245411Z","shell.execute_reply.started":"2024-06-01T03:34:20.231644Z","shell.execute_reply":"2024-06-01T03:34:20.244451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:20.246703Z","iopub.execute_input":"2024-06-01T03:34:20.24704Z","iopub.status.idle":"2024-06-01T03:34:20.254828Z","shell.execute_reply.started":"2024-06-01T03:34:20.247009Z","shell.execute_reply":"2024-06-01T03:34:20.253877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:20.25609Z","iopub.execute_input":"2024-06-01T03:34:20.256402Z","iopub.status.idle":"2024-06-01T03:34:20.26247Z","shell.execute_reply.started":"2024-06-01T03:34:20.256379Z","shell.execute_reply":"2024-06-01T03:34:20.261547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.text = df.text.apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:34:20.266376Z","iopub.execute_input":"2024-06-01T03:34:20.266659Z","iopub.status.idle":"2024-06-01T03:35:19.589042Z","shell.execute_reply.started":"2024-06-01T03:34:20.266637Z","shell.execute_reply":"2024-06-01T03:35:19.588261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Aaww.. It is clean and tidy now. Now let's see some word cloud visualizations of it.**\n\n### Positive Words","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:35:19.590026Z","iopub.execute_input":"2024-06-01T03:35:19.590277Z","iopub.status.idle":"2024-06-01T03:36:22.618941Z","shell.execute_reply.started":"2024-06-01T03:35:19.590257Z","shell.execute_reply":"2024-06-01T03:36:22.61768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Negative Words","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:36:22.620302Z","iopub.execute_input":"2024-06-01T03:36:22.620835Z","iopub.status.idle":"2024-06-01T03:37:25.734767Z","shell.execute_reply.started":"2024-06-01T03:36:22.620807Z","shell.execute_reply":"2024-06-01T03:37:25.733714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ› Train and Test SplitðŸ›  </div>","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 30","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:40:36.682097Z","iopub.execute_input":"2024-06-19T10:40:36.682959Z","iopub.status.idle":"2024-06-19T10:40:36.687052Z","shell.execute_reply.started":"2024-06-19T10:40:36.682924Z","shell.execute_reply":"2024-06-19T10:40:36.686150Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n                                         random_state=7) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size\", len(test_data))","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:37:25.741339Z","iopub.execute_input":"2024-06-01T03:37:25.742042Z","iopub.status.idle":"2024-06-01T03:37:26.001612Z","shell.execute_reply.started":"2024-06-01T03:37:25.742014Z","shell.execute_reply":"2024-06-01T03:37:26.000698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`train_test_split` will shuffle the dataset and split it to gives training and testing dataset. It's important to shuffle our dataset before training.","metadata":{}},{"cell_type":"code","source":"train_data.sample(10).style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'}) # Returns the rows with the index and display it","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:37:26.00287Z","iopub.execute_input":"2024-06-01T03:37:26.003226Z","iopub.status.idle":"2024-06-01T03:37:26.043768Z","shell.execute_reply.started":"2024-06-01T03:37:26.003193Z","shell.execute_reply":"2024-06-01T03:37:26.042863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ› TokenizationðŸ›  </div>","metadata":{}},{"cell_type":"markdown","source":"Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called *tokens* , perhaps at the same time throwing away certain characters, such as punctuation. The process is called **Tokenization.**\n![Tokenization](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png)\n\n`tokenizer` create tokens for every word in the data corpus and map them to a index using dictionary.\n\n`word_index` contains the index for each word\n\n`vocab_size` represents the total number of word in the data corpus","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:37:26.045067Z","iopub.execute_input":"2024-06-01T03:37:26.045812Z","iopub.status.idle":"2024-06-01T03:37:46.811241Z","shell.execute_reply.started":"2024-06-01T03:37:26.045779Z","shell.execute_reply":"2024-06-01T03:37:46.81032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we got a `tokenizer` object, which can be used to covert any word into a Key in dictionary (number).\n\nSince we are going to build a sequence model. We should feed in a sequence of numbers to it. And also we should ensure there is no variance in input shapes of sequences. It all should be of same lenght. But texts in tweets have different count of words in it. To avoid this, we seek a little help from `pad_sequence` to do our job. It will make all the sequence in one constant length `MAX_SEQUENCE_LENGTH`.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:37:46.812382Z","iopub.execute_input":"2024-06-01T03:37:46.81265Z","iopub.status.idle":"2024-06-01T03:38:15.829657Z","shell.execute_reply.started":"2024-06-01T03:37:46.812629Z","shell.execute_reply":"2024-06-01T03:38:15.828575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_data.sentiment.unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:38:15.830925Z","iopub.execute_input":"2024-06-01T03:38:15.831249Z","iopub.status.idle":"2024-06-01T03:38:15.929963Z","shell.execute_reply.started":"2024-06-01T03:38:15.831225Z","shell.execute_reply":"2024-06-01T03:38:15.929119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> Label EncodingðŸ›  </div>","metadata":{}},{"cell_type":"markdown","source":"We are building the model to predict class in enocoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings.","metadata":{}},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:38:15.931135Z","iopub.execute_input":"2024-06-01T03:38:15.931492Z","iopub.status.idle":"2024-06-01T03:38:17.869976Z","shell.execute_reply.started":"2024-06-01T03:38:15.931461Z","shell.execute_reply":"2024-06-01T03:38:17.868994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ› Word EmbeddingðŸ›  </div>","metadata":{}},{"cell_type":"markdown","source":"# Word Emdedding\nIn Language Model, words are represented in a way to intend more meaning and for learning the patterns and contextual meaning behind it. \n\n**Word Embedding** is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\nBasically, it's a feature vector representation of words which are used for other natural language processing applications.\n\nWe could train the embedding ourselves but that would take a while to train and it wouldn't be effective. So going in the path of Computer Vision, here we use **Transfer Learning**. We download the pre-trained embedding and use it in our model.\n\nThe pretrained Word Embedding like **GloVe & Word2Vec** gives more insights for a word which can be used for classification. If you want to learn more about the Word Embedding, please refer some links that I left at the end of this notebook.\n\n\nIn this notebook, I use **GloVe Embedding from Stanford AI** which can be found [here](https://nlp.stanford.edu/projects/glove/)","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:38:17.873471Z","iopub.execute_input":"2024-06-01T03:38:17.873769Z","iopub.status.idle":"2024-06-01T03:41:19.149144Z","shell.execute_reply.started":"2024-06-01T03:38:17.873745Z","shell.execute_reply":"2024-06-01T03:41:19.148158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 50\nMODEL_PATH = '.../output/kaggle/working/best_model.hdf5'","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:41:19.15067Z","iopub.execute_input":"2024-06-01T03:41:19.150959Z","iopub.status.idle":"2024-06-01T03:41:19.155905Z","shell.execute_reply.started":"2024-06-01T03:41:19.150934Z","shell.execute_reply":"2024-06-01T03:41:19.155023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:41:19.157137Z","iopub.execute_input":"2024-06-01T03:41:19.157569Z","iopub.status.idle":"2024-06-01T03:41:51.125669Z","shell.execute_reply.started":"2024-06-01T03:41:19.157538Z","shell.execute_reply":"2024-06-01T03:41:51.124715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:41:51.126913Z","iopub.execute_input":"2024-06-01T03:41:51.127209Z","iopub.status.idle":"2024-06-01T03:41:51.661252Z","shell.execute_reply.started":"2024-06-01T03:41:51.127183Z","shell.execute_reply":"2024-06-01T03:41:51.660493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:40:57.382108Z","iopub.execute_input":"2024-06-19T10:40:57.382666Z","iopub.status.idle":"2024-06-19T10:40:57.422463Z","shell.execute_reply.started":"2024-06-19T10:40:57.382630Z","shell.execute_reply":"2024-06-19T10:40:57.421181Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size,\n\u001b[1;32m      2\u001b[0m                                           EMBEDDING_DIM,\n\u001b[1;32m      3\u001b[0m                                           weights\u001b[38;5;241m=\u001b[39m[embedding_matrix],\n\u001b[1;32m      4\u001b[0m                                           input_length\u001b[38;5;241m=\u001b[39mMAX_SEQUENCE_LENGTH,\n\u001b[1;32m      5\u001b[0m                                           trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"],"ename":"NameError","evalue":"name 'tf' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ”¥Model Training - LSTMðŸ”¥ </div>","metadata":{}},{"cell_type":"markdown","source":"We are clear to build our Deep Learning model. While developing a DL model, we should keep in mind of key things like Model Architecture, Hyperparmeter Tuning and Performance of the model.\n\nAs you can see in the word cloud, the some words are predominantly feature in both Positive and Negative tweets. This could be a problem if we are using a Machine Learning model like Naive Bayes, SVD, etc.. That's why we use **Sequence Models**.\n\n### Sequence Model\n![Sequence Model](https://miro.medium.com/max/1458/1*SICYykT7ybua1gVJDNlajw.png)\n\nReccurent Neural Networks can handle a seqence of data and learn a pattern of input seqence to give either sequence or scalar value as output. In our case, the Neural Network outputs a scalar value prediction. \n\nFor model architecture, we use\n\n1) **Embedding Layer** - Generates Embedding Vector for each input sequence.\n\n2) **Conv1D Layer** - Its using to convolve data into smaller feature vectors. \n\n3) **LSTM** - Long Short Term Memory, its a variant of RNN which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN.\n\n4) **Dense** - Fully Connected Layers for classification\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:40:02.770884Z","iopub.execute_input":"2024-06-19T10:40:02.771165Z","iopub.status.idle":"2024-06-19T10:40:15.030167Z","shell.execute_reply.started":"2024-06-19T10:40:02.771139Z","shell.execute_reply":"2024-06-19T10:40:15.029222Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-19 10:40:04.455560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-19 10:40:04.455649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-19 10:40:04.580955: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:40:44.624908Z","iopub.execute_input":"2024-06-19T10:40:44.625264Z","iopub.status.idle":"2024-06-19T10:40:44.667208Z","shell.execute_reply.started":"2024-06-19T10:40:44.625226Z","shell.execute_reply":"2024-06-19T10:40:44.666075Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sequence_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(MAX_SEQUENCE_LENGTH,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m embedding_sequences \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m(sequence_input)\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m SpatialDropout1D(\u001b[38;5;241m0.2\u001b[39m)(embedding_sequences)\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv1D(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n","\u001b[0;31mNameError\u001b[0m: name 'embedding_layer' is not defined"],"ename":"NameError","evalue":"name 'embedding_layer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:41:53.807145Z","iopub.execute_input":"2024-06-01T03:41:53.807413Z","iopub.status.idle":"2024-06-01T03:41:54.104187Z","shell.execute_reply.started":"2024-06-01T03:41:53.807391Z","shell.execute_reply":"2024-06-01T03:41:54.103371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimization Algorithm\nThis notebook uses Adam, optimization algorithm for Gradient Descent. You can learn more about Adam [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n\n### Callbacks\nCallbacks are special functions which are called at the end of an epoch. We can use any functions to perform specific operation after each epoch. I used two callbacks here,\n\n- **LRScheduler** - It changes a Learning Rate at specfic epoch to achieve more improved result. In this notebook, the learning rate exponentionally decreases after remaining same for first 10 Epoch.\n\n- **ModelCheckPoint** - It saves best model while training based on some metrics. Here, it saves the model with minimum Validity Loss.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:41:54.105254Z","iopub.execute_input":"2024-06-01T03:41:54.105565Z","iopub.status.idle":"2024-06-01T03:41:54.124051Z","shell.execute_reply.started":"2024-06-01T03:41:54.105541Z","shell.execute_reply":"2024-06-01T03:41:54.123352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start training... It takes a heck of a time if training in CPU, be sure your GPU turned on... May the CUDA Cores be with you....","metadata":{}},{"cell_type":"code","source":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:41:54.125102Z","iopub.execute_input":"2024-06-01T03:41:54.125394Z","iopub.status.idle":"2024-06-01T03:41:54.132948Z","shell.execute_reply.started":"2024-06-01T03:41:54.125369Z","shell.execute_reply":"2024-06-01T03:41:54.132085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2024-06-01T03:41:54.13398Z","iopub.execute_input":"2024-06-01T03:41:54.134259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ“ˆModel EvaluationðŸ’¹ </div>","metadata":{}},{"cell_type":"markdown","source":"Now that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch.","metadata":{}},{"cell_type":"code","source":"s, (at, al) = plt.subplots(2,1)\nat.plot(history.history['accuracy'], c= 'b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment.","metadata":{}},{"cell_type":"code","source":"def decode_sentiment(score):\n    return \"Positive\" if score>0.5 else \"Negative\"\n\n\nscores = model.predict(x_test, verbose=1, batch_size=10000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ§¾Confusion MatrixðŸ§¾ </div>","metadata":{}},{"cell_type":"markdown","source":"Confusion Matrix provide a nice overlook at the model's performance in classification task","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  <div style=\"color:#FF5733; background-color:#F7DC6F; padding: 20px; border-radius: 15px; font-size: 200%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align:center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\"> ðŸ“œClassification ScoreðŸ“œ </div>","metadata":{}},{"cell_type":"code","source":"print(classification_report(list(test_data.sentiment), y_pred_1d))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}